{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "463ae2ff-2580-4da6-9a54-d3b05bb44c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 사용할 관절(Keypoints)\n",
    "# keypoints = [\n",
    "#     \"Nose\", \"Left Eye\", \"Right Eye\", \"Left Ear\", \"Right Ear\",\n",
    "#     \"Left Shoulder\", \"Right Shoulder\", \"Left Elbow\", \"Right Elbow\",\n",
    "#     \"Left Wrist\", \"Right Wrist\", \"Left Hip\", \"Right Hip\",\n",
    "#     \"Left Knee\", \"Right Knee\", \"Left Ankle\", \"Right Ankle\",\n",
    "#     \"Neck\", \"Left Palm\", \"Right Palm\", \"Back\", \"Waist\",\n",
    "#     \"Left Foot\", \"Right Foot\"\n",
    "# ]\n",
    "0, 2, 5, 7, 8, 11, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28\n",
    "keypoints = [\n",
    "    \"Point_0\", \"Point_2\", \"Point_5\", \"Point_7\", \"Point_8\", \"Point_11\", \n",
    "    \"Point_12\", \"Point_13\", \"Point_14\", \"Point_15\", \"Point_16\", \"Point_21\", \n",
    "    \"Point_22\", \"Point_23\", \"Point_24\", \"Point_25\", \"Point_26\", \"Point_27\", \"Point_28\"\n",
    "]\n",
    "\n",
    "\n",
    "# JSON 데이터 로드 함수\n",
    "def load_json_skeleton_view1(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    num_frames = len(data[\"frames\"])\n",
    "    num_joints = len(keypoints)\n",
    "    num_features = 2  # (x, y)\n",
    "    num_views = 1     \n",
    "\n",
    "    # ✅ (1, 프레임, 뷰, 관절, 좌표) 형태로 데이터 배열 생성\n",
    "    X_data = np.zeros((1, num_frames, num_views, num_joints, num_features), dtype=np.float32)\n",
    "\n",
    "    views = [\"view1\"]\n",
    "\n",
    "    # ✅ JSON 데이터 -> 배열 변환\n",
    "    for frame_idx, frame in enumerate(data[\"frames\"]):\n",
    "        for view_idx, view in enumerate(views):\n",
    "            pts = frame.get(view, {}).get(\"pts\", {})\n",
    "            for joint_idx, joint_name in enumerate(keypoints):\n",
    "                if joint_name in pts:\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 0] = pts[joint_name][\"x\"]\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 1] = pts[joint_name][\"y\"]\n",
    "\n",
    "    return X_data, data.get(\"type_info\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "b1c40f17-54a1-4684-a426-716833db587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 데이터 Shape: (80, 32, 5, 19, 2)\n"
     ]
    }
   ],
   "source": [
    "# ✅ JSON 데이터 로드 함수 (5개 각도 전처리)\n",
    "def load_json_skeleton(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    num_frames = len(data[\"frames\"])\n",
    "    num_joints = len(keypoints)\n",
    "    num_features = 2  # (x, y)\n",
    "    num_views = 5     # view1 ~ view5\n",
    "\n",
    "    # ✅ (1, 프레임, 뷰, 관절, 좌표) 형태로 데이터 배열 생성\n",
    "    X_data = np.zeros((1, num_frames, num_views, num_joints, num_features), dtype=np.float32)\n",
    "\n",
    "    views = [\"view1\", \"view2\", \"view3\", \"view4\", \"view5\"]\n",
    "\n",
    "    # ✅ JSON 데이터 -> 배열 변환\n",
    "    for frame_idx, frame in enumerate(data[\"frames\"]):\n",
    "        for view_idx, view in enumerate(views):\n",
    "            pts = frame.get(view, {}).get(\"pts\", {})\n",
    "            for joint_idx, joint_name in enumerate(keypoints):\n",
    "                if joint_name in pts:\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 0] = pts[joint_name][\"x\"]\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 1] = pts[joint_name][\"y\"]\n",
    "\n",
    "    return X_data, data.get(\"type_info\", None)\n",
    "\n",
    "# ✅ 여러 개의 JSON 파일을 한 번에 로드하는 함수 (올바른/잘못된 데이터 포함)\n",
    "def load_labeled_json_skeleton(file_paths, labels):\n",
    "    X_data_list = []\n",
    "    y_data_list = []\n",
    "\n",
    "    for file_path, label in zip(file_paths, labels):\n",
    "        X, _ = load_json_skeleton(file_path)\n",
    "        X_data_list.append(X)\n",
    "        y_data_list.append(label)\n",
    "\n",
    "    # ✅ 여러 개의 파일을 하나의 NumPy 배열로 병합\n",
    "    X_train = np.concatenate(X_data_list, axis=0)  # (batch_size, frames, views, joints, features)\n",
    "    y_train = np.array(y_data_list)                # (batch_size, )\n",
    "\n",
    "    return X_train, y_train\n",
    "    \n",
    "# ✅ 올바른 자세와 잘못된 자세 데이터를 함께 로드\n",
    "file_paths = [\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body17-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body17-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body17-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body17-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body17-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-562.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-563.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-564.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-565.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-566.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-567.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-568.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-569.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-570.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-571.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-572.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-573.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-574.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-575.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-576.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-577.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-578.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-579.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-580.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-581.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-582.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-583.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-584.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-585.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-586.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-587.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-588.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-589.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-590.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-591.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-592.json\",    #31\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-562.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-563.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-564.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-565.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-566.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-567.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-568.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-569.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-570.json\"\n",
    "]\n",
    "\n",
    "labels = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,]  # 0 = 올바른 자세, 1 = 잘못된 자세\n",
    "\n",
    "# ✅ 전처리 실행\n",
    "X_train, y_train = load_labeled_json_skeleton(file_paths, labels)\n",
    "# ✅ 전처리된 데이터 형태 확인\n",
    "print(\"전처리된 데이터 Shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "eb3eaa30-3fdb-4a25-ae7e-7692c4e52f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 생성된 adjacency_matrix 크기: (19, 19)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import mediapipe as mp\n",
    "\n",
    "# ✅ 그래프 컨볼루션 레이어 정의\n",
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(self, units, adjacency_matrix):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.adjacency_matrix = tf.Variable(adjacency_matrix, dtype=tf.float32, trainable=False)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.linalg.matmul(self.adjacency_matrix, inputs)  # (batch, joints, features)\n",
    "        x = tf.linalg.matmul(x, self.kernel)  # 가중치 적용\n",
    "        return tf.nn.leaky_relu(x)  # 활성화 함수 적용\n",
    "\n",
    "# ✅ ST-GCN 모델 정의\n",
    "class STGCN(tf.keras.Model):\n",
    "    def __init__(self, num_joints, num_features, adjacency_matrix, num_classes):\n",
    "        super(STGCN, self).__init__()\n",
    "        self.graph_conv1 = GraphConvLayer(64, adjacency_matrix)\n",
    "        self.graph_conv2 = GraphConvLayer(128, adjacency_matrix)\n",
    "        self.graph_conv3 = GraphConvLayer(256, adjacency_matrix)  # 추가된 Graph Conv\n",
    "        self.graph_conv4 = GraphConvLayer(512, adjacency_matrix)  # 추가된 Graph Conv\n",
    "        self.temporal_conv1 = layers.Conv1D(512, kernel_size=5, padding=\"same\")\n",
    "        self.temporal_conv2 = layers.Conv1D(256, kernel_size=5, padding=\"same\")\n",
    "        self.temporal_conv3 = layers.Conv1D(128, kernel_size=5, padding=\"same\")\n",
    "        self.temporal_conv4 = layers.Conv1D(64, kernel_size=5, padding=\"same\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "        self.batch_norm3 = layers.BatchNormalization()\n",
    "        self.batch_norm4 = layers.BatchNormalization()\n",
    "        self.activation = layers.Activation(\"relu\")\n",
    "        self.fc = layers.Dense(num_classes, activation=\"softmax\")\n",
    "        self.dropout = layers.Dropout(0.5) \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # ✅ 입력 처리: (batch, frames, views, joints, features)\n",
    "        if len(inputs.shape) == 5:\n",
    "            # 여러 각도(View) 데이터가 있는 경우 평균 내기\n",
    "            inputs = tf.reduce_mean(inputs, axis=2)  # (batch, frames, joints, features)\n",
    "        \n",
    "        batch_size, frames, joints, features = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n",
    "       \n",
    "        # inputs = tf.reshape(inputs, (batch_size, frames, joints * features))\n",
    "        \n",
    "        # # ✅ 차원 재정렬: (batch, joints, frames, features)\n",
    "        # inputs = tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "        # inputs = tf.reshape(inputs, (batch_size, joints, frames * features))\n",
    "\n",
    "        # ✅ 차원 변환 (batch, frames, joints, features) → (batch * frames, joints, features)\n",
    "        batch_size, frames, joints, features = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n",
    "        x = tf.reshape(inputs, (batch_size * frames, joints, features))  # (batch * frames, joints, features)\n",
    "\n",
    "        # ✅ 모델 처리\n",
    "        x = self.graph_conv1(inputs)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.graph_conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.graph_conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.graph_conv4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # ✅ Graph Conv 이후 frames 차원 유지 (batch, frames, features)\n",
    "        x = tf.reshape(x, (batch_size, frames, -1))\n",
    "\n",
    "        x = self.temporal_conv1(x)\n",
    "        x = self.temporal_conv2(x)\n",
    "        x = self.temporal_conv3(x)\n",
    "        x = self.temporal_conv4(x)\n",
    "\n",
    "        # ✅ frames 차원을 평균 내어 `batch_size`만 남기기\n",
    "        x = tf.reduce_mean(x, axis=1)  # (batch, features)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.fc(x)\n",
    "\n",
    "# ✅ 그래프 인접 행렬 (단순 단위 행렬)\n",
    "num_joints = len(keypoints)\n",
    "num_features = 2\n",
    "num_classes = 2  # (올바른 자세 / 잘못된 자세)\n",
    "# adjacency_matrix = np.identity(num_joints)\n",
    "\n",
    "# Mediapipe에서 제공하는 POSE_CONNECTIONS을 활용\n",
    "mp_pose = mp.solutions.pose\n",
    "connections = list(mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "# ✅ 현재 선택된 19개 관절 인덱스 (사용자가 선택한 관절 리스트)\n",
    "selected_joints = [0, 2, 5, 7, 8, 11, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "# ✅ 원본 33개 관절을 기반으로 한 인접 행렬 생성\n",
    "full_adjacency_matrix = np.zeros((33, 33))  # 전체 33개 관절을 사용한 경우\n",
    "for joint1, joint2 in connections:\n",
    "    full_adjacency_matrix[joint1, joint2] = 1\n",
    "    full_adjacency_matrix[joint2, joint1] = 1  # 대칭 관계\n",
    "\n",
    "# ✅ 선택된 19개 관절만을 포함하는 인접 행렬 생성\n",
    "num_joints = len(selected_joints)\n",
    "adjacency_matrix = np.zeros((num_joints, num_joints))\n",
    "\n",
    "for i, joint1 in enumerate(selected_joints):\n",
    "    for j, joint2 in enumerate(selected_joints):\n",
    "        adjacency_matrix[i, j] = full_adjacency_matrix[joint1, joint2]  # 기존 인접 행렬에서 추출\n",
    "\n",
    "print(f\"✅ 생성된 adjacency_matrix 크기: {adjacency_matrix.shape}\")  # (19, 19) 확인\n",
    "\n",
    "# ✅ ST-GCN 모델 생성 및 컴파일\n",
    "del stgcn_model\n",
    "stgcn_model = STGCN(num_joints, num_features, adjacency_matrix, num_classes)\n",
    "stgcn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "5725513f-47a7-48ec-bfb8-265738ac83ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.5000 - loss: 263.1748\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 2430.9277\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 2864.7664\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 2421.9285\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 1640.8099\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 752.1889\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5125 - loss: 426.5847\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5125 - loss: 347.2073\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 215.1311\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 382.1622\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[509], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m stgcn_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m stgcn_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      8\u001b[0m stgcn_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1684\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1685\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1686\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1687\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1688\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1689\u001b[0m   )\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "stgcn_model.fit(X_train, y_train, epochs=10, batch_size=80, verbose=1)\n",
    "stgcn_model.fit(X_train, y_train, epochs=1000, batch_size=80, verbose=0)\n",
    "stgcn_model.fit(X_train, y_train, epochs=10, batch_size=80, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb185c6a-391e-489c-aa8c-94ef81e2b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 여러 개의 JSON 파일을 로드하고 모델 예측 수행\n",
    "def predict_multiple_json_skeleton(file_paths):\n",
    "    results = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # JSON 데이터 로드\n",
    "        X_data, _ = load_json_skeleton(file_path)  # 기존의 JSON 로딩 함수 사용\n",
    "\n",
    "        \n",
    "        # # ✅ View 차원 평균 제거 및 변환을 `fit()` 실행 전에 처리\n",
    "        # X_data = np.mean(X_data, axis=2)  # (80, 32, 19, 2)\n",
    "        # X_data = np.transpose(X_data, (0, 2, 1, 3))  # (80, 19, 32, 2)\n",
    "        # batch_size, joints, frames, features = X_data.shape\n",
    "        # X_data = np.reshape(X_data, (batch_size, joints, frames * features))  # (80, 19, 64)\n",
    "        \n",
    "        # # ✅ Tensor 변환\n",
    "        # X_data = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "\n",
    "        # 모델 예측\n",
    "        prediction = stgcn_model.predict(X_data)\n",
    "        predicted_class = np.argmax(prediction, axis=1)[0]  # 0 = 올바른 자세, 1 = 잘못된 자세\n",
    "        confidence = prediction[0][predicted_class]  # 선택된 클래스의 확률 값\n",
    "\n",
    "        # ✅ 결과 저장\n",
    "        if predicted_class == 0:\n",
    "            results[file_path] = f\"✅ 올바른 자세 ({confidence * 100:.2f}% 확신)\"\n",
    "        else:\n",
    "            results[file_path] = f\"❌ 잘못된 자세 감지 ({confidence * 100:.2f}% 확신)\"\n",
    "\n",
    "        #     # ✅ 결과 저장\n",
    "        # if predicted_class == 0:\n",
    "        #     results[file_path] = f\"✅ 올바른 자세\"\n",
    "        # else:\n",
    "        #     results[file_path] = f\"❌ 잘못된 자세 감지\"\n",
    "\n",
    "    return results\n",
    "\n",
    "# ✅ 여러 개의 JSON 파일 리스트\n",
    "file_paths = [\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-562.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-563.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-564.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-565.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-566.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-567.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-568.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-569.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-570.json\"\n",
    "]\n",
    "\n",
    "# ✅ 예측 결과 얻기\n",
    "prediction_results = predict_multiple_json_skeleton(file_paths)\n",
    "\n",
    "# ✅ 결과 출력\n",
    "for file, result in prediction_results.items():\n",
    "    print(f\"{file}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
