{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463ae2ff-2580-4da6-9a54-d3b05bb44c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 사용할 관절(Keypoints)\n",
    "# keypoints = [\n",
    "#     \"Nose\", \"Left Eye\", \"Right Eye\", \"Left Ear\", \"Right Ear\",\n",
    "#     \"Left Shoulder\", \"Right Shoulder\", \"Left Elbow\", \"Right Elbow\",\n",
    "#     \"Left Wrist\", \"Right Wrist\", \"Left Hip\", \"Right Hip\",\n",
    "#     \"Left Knee\", \"Right Knee\", \"Left Ankle\", \"Right Ankle\",\n",
    "#     \"Neck\", \"Left Palm\", \"Right Palm\", \"Back\", \"Waist\",\n",
    "#     \"Left Foot\", \"Right Foot\"\n",
    "# ]\n",
    "0, 2, 5, 7, 8, 11, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28\n",
    "keypoints = [\n",
    "    \"Point_0\", \"Point_2\", \"Point_5\", \"Point_7\", \"Point_8\", \"Point_11\", \n",
    "    \"Point_12\", \"Point_13\", \"Point_14\", \"Point_15\", \"Point_16\", \"Point_21\", \n",
    "    \"Point_22\", \"Point_23\", \"Point_24\", \"Point_25\", \"Point_26\", \"Point_27\", \"Point_28\"\n",
    "]\n",
    "\n",
    "\n",
    "# JSON 데이터 로드 함수\n",
    "def load_json_skeleton_view1(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    num_frames = len(data[\"frames\"])\n",
    "    num_joints = len(keypoints)\n",
    "    num_features = 2  # (x, y)\n",
    "    num_views = 1     \n",
    "\n",
    "    # ✅ (1, 프레임, 뷰, 관절, 좌표) 형태로 데이터 배열 생성\n",
    "    X_data = np.zeros((1, num_frames, num_views, num_joints, num_features), dtype=np.float32)\n",
    "\n",
    "    views = [\"view1\"]\n",
    "\n",
    "    # ✅ JSON 데이터 -> 배열 변환\n",
    "    for frame_idx, frame in enumerate(data[\"frames\"]):\n",
    "        for view_idx, view in enumerate(views):\n",
    "            pts = frame.get(view, {}).get(\"pts\", {})\n",
    "            for joint_idx, joint_name in enumerate(keypoints):\n",
    "                if joint_name in pts:\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 0] = pts[joint_name][\"x\"]\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 1] = pts[joint_name][\"y\"]\n",
    "\n",
    "    return X_data, data.get(\"type_info\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1c40f17-54a1-4684-a426-716833db587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 데이터 Shape: (80, 32, 5, 19, 2)\n"
     ]
    }
   ],
   "source": [
    "# ✅ JSON 데이터 로드 함수 (5개 각도 전처리)\n",
    "def load_json_skeleton(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    num_frames = len(data[\"frames\"])\n",
    "    num_joints = len(keypoints)\n",
    "    num_features = 2  # (x, y)\n",
    "    num_views = 5     # view1 ~ view5\n",
    "\n",
    "    # ✅ (1, 프레임, 뷰, 관절, 좌표) 형태로 데이터 배열 생성\n",
    "    X_data = np.zeros((1, num_frames, num_views, num_joints, num_features), dtype=np.float32)\n",
    "\n",
    "    views = [\"view1\", \"view2\", \"view3\", \"view4\", \"view5\"]\n",
    "\n",
    "    # ✅ JSON 데이터 -> 배열 변환\n",
    "    for frame_idx, frame in enumerate(data[\"frames\"]):\n",
    "        for view_idx, view in enumerate(views):\n",
    "            pts = frame.get(view, {}).get(\"pts\", {})\n",
    "            for joint_idx, joint_name in enumerate(keypoints):\n",
    "                if joint_name in pts:\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 0] = pts[joint_name][\"x\"]\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 1] = pts[joint_name][\"y\"]\n",
    "\n",
    "    return X_data, data.get(\"type_info\", None)\n",
    "\n",
    "# ✅ 여러 개의 JSON 파일을 한 번에 로드하는 함수 (올바른/잘못된 데이터 포함)\n",
    "def load_labeled_json_skeleton(file_paths, labels):\n",
    "    X_data_list = []\n",
    "    y_data_list = []\n",
    "\n",
    "    for file_path, label in zip(file_paths, labels):\n",
    "        X, _ = load_json_skeleton(file_path)\n",
    "        X_data_list.append(X)\n",
    "        y_data_list.append(label)\n",
    "\n",
    "    # ✅ 여러 개의 파일을 하나의 NumPy 배열로 병합\n",
    "    X_train = np.concatenate(X_data_list, axis=0)  # (batch_size, frames, views, joints, features)\n",
    "    y_train = np.array(y_data_list)                # (batch_size, )\n",
    "\n",
    "    return X_train, y_train\n",
    "    \n",
    "# ✅ 올바른 자세와 잘못된 자세 데이터를 함께 로드\n",
    "file_paths = [\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body10-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body10-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body10-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body10-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body10-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body10-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body10-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body11-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body11-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body11-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body11-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body11-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body11-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body11-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body12-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body12-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body12-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body12-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body12-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body12-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body12-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body17-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body17-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body17-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body17-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body17-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-562.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-563.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-564.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-565.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-566.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-567.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-568.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-569.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-570.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-571.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-572.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-573.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-574.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-575.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-576.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-577.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-578.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-579.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-580.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-581.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-582.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-583.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-584.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-585.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-586.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-587.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-588.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-589.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-590.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-591.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body09-1-592.json\",    #31\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-1-562.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-1-563.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-1-564.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-1-565.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-1-566.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-1-567.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-1-568.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-1-569.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/body08-1-570.json\"\n",
    "]\n",
    "\n",
    "labels = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,]  # 0 = 올바른 자세, 1 = 잘못된 자세\n",
    "\n",
    "# ✅ 전처리 실행\n",
    "X_train, y_train = load_labeled_json_skeleton(file_paths, labels)\n",
    "# X_train = X_train / np.max(X_train)\n",
    "# ✅ 전처리된 데이터 형태 확인\n",
    "print(\"전처리된 데이터 Shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb3eaa30-3fdb-4a25-ae7e-7692c4e52f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ✅ 그래프 컨볼루션 레이어 정의\n",
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(self, units, adjacency_matrix):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.adjacency_matrix = tf.convert_to_tensor(adjacency_matrix, dtype=tf.float32)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.linalg.matmul(self.adjacency_matrix, inputs)  # 그래프 구조 반영\n",
    "        x = tf.linalg.matmul(x, self.kernel)  # 가중치 적용\n",
    "        return tf.nn.leaky_relu(x)  # 활성화 함수 적용\n",
    "\n",
    "# ✅ ST-GCN 모델 정의\n",
    "class STGCN(tf.keras.Model):\n",
    "    def __init__(self, num_joints, num_features, adjacency_matrix, num_classes):\n",
    "        super(STGCN, self).__init__()\n",
    "        self.graph_conv1 = GraphConvLayer(64, adjacency_matrix)\n",
    "        self.graph_conv2 = GraphConvLayer(128, adjacency_matrix)\n",
    "        self.temporal_conv1 = layers.Conv1D(128, kernel_size=3, padding=\"same\")\n",
    "        self.temporal_conv2 = layers.Conv1D(64, kernel_size=3, padding=\"same\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "        self.activation = layers.Activation(\"relu\")\n",
    "        self.fc = layers.Dense(num_classes, activation=\"softmax\")\n",
    "        self.dropout = layers.Dropout(0.5) \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # ✅ 입력 처리: (batch, frames, views, joints, features)\n",
    "        if len(inputs.shape) == 5:\n",
    "            # 여러 각도(View) 데이터가 있는 경우 평균 내기\n",
    "            inputs = tf.reduce_mean(inputs, axis=2)  # (batch, frames, joints, features)\n",
    "        \n",
    "        batch_size, frames, joints, features = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n",
    "       \n",
    "        # inputs = tf.reshape(inputs, (batch_size, frames, joints * features))\n",
    "        \n",
    "        # ✅ 차원 재정렬: (batch, joints, frames, features)\n",
    "        inputs = tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "        inputs = tf.reshape(inputs, (batch_size, joints, frames * features))\n",
    "\n",
    "        # ✅ 모델 처리\n",
    "        x = self.graph_conv1(inputs)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.graph_conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.temporal_conv1(x)\n",
    "        x = self.temporal_conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.fc(x)\n",
    "\n",
    "# ✅ 그래프 인접 행렬 (단순 단위 행렬)\n",
    "num_joints = len(keypoints)\n",
    "num_features = 2\n",
    "num_classes = 2  # (올바른 자세 / 잘못된 자세)\n",
    "adjacency_matrix = np.identity(num_joints)\n",
    "\n",
    "# ✅ ST-GCN 모델 생성 및 컴파일\n",
    "del stgcn_model\n",
    "stgcn_model = STGCN(num_joints, num_features, adjacency_matrix, num_classes)\n",
    "stgcn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5725513f-47a7-48ec-bfb8-265738ac83ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.9096\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7041\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.7335\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.7814\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7606\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7166\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6884\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.6875\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7017\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7126\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.2901e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.2872e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.2843e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.2814e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.2786e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.2758e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.2730e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.2701e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.2673e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.2644e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b7cb3e31a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "stgcn_model.fit(X_train, y_train, epochs=10, batch_size=80, verbose=1)\n",
    "stgcn_model.fit(X_train, y_train, epochs=1000, batch_size=80, verbose=0)\n",
    "stgcn_model.fit(X_train, y_train, epochs=10, batch_size=80, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb185c6a-391e-489c-aa8c-94ef81e2b9f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling GraphConvLayer.call().\n\n\u001b[1mDimensions must be equal, but are 32 and 64 for '{{node stgcn_2_1/graph_conv_layer_4_1/MatMul_1}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false, grad_x=false, grad_y=false](stgcn_2_1/graph_conv_layer_4_1/MatMul, stgcn_2_1/graph_conv_layer_4_1/MatMul_1/ReadVariableOp)' with input shapes: [1,19,32], [64,64].\u001b[0m\n\nArguments received by GraphConvLayer.call():\n  • inputs=tf.Tensor(shape=(1, 19, 32), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 71\u001b[0m\n\u001b[0;32m     29\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Studying/gradu/013.피트니스자세/2.Validation/라벨링데이터/body_01/Day32_201104_F/D32-1-561.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Studying/gradu/013.피트니스자세/2.Validation/라벨링데이터/body_01/Day32_201104_F/D32-2-561.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Studying/gradu/013.피트니스자세/2.Validation/라벨링데이터/body_01/Day32_201104_F/D32-1-592.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m ]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# ✅ 예측 결과 얻기\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m prediction_results \u001b[38;5;241m=\u001b[39m predict_multiple_json_skeleton(file_paths)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# ✅ 결과 출력\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file, result \u001b[38;5;129;01min\u001b[39;00m prediction_results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m, in \u001b[0;36mpredict_multiple_json_skeleton\u001b[1;34m(file_paths)\u001b[0m\n\u001b[0;32m      7\u001b[0m X_data, _ \u001b[38;5;241m=\u001b[39m load_json_skeleton_view1(file_path)  \u001b[38;5;66;03m# 기존의 JSON 로딩 함수 사용\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 모델 예측\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m prediction \u001b[38;5;241m=\u001b[39m stgcn_model\u001b[38;5;241m.\u001b[39mpredict(X_data)\n\u001b[0;32m     11\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# 0 = 올바른 자세, 1 = 잘못된 자세\u001b[39;00m\n\u001b[0;32m     12\u001b[0m confidence \u001b[38;5;241m=\u001b[39m prediction[\u001b[38;5;241m0\u001b[39m][predicted_class]  \u001b[38;5;66;03m# 선택된 클래스의 확률 값\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[18], line 56\u001b[0m, in \u001b[0;36mSTGCN.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     53\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(inputs, (batch_size, joints, frames \u001b[38;5;241m*\u001b[39m features))\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# ✅ 모델 처리\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_conv1(inputs)\n\u001b[0;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm1(x)\n\u001b[0;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n",
      "Cell \u001b[1;32mIn[18], line 20\u001b[0m, in \u001b[0;36mGraphConvLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjacency_matrix, inputs)  \u001b[38;5;66;03m# 그래프 구조 반영\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel)  \u001b[38;5;66;03m# 가중치 적용\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mleaky_relu(x)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling GraphConvLayer.call().\n\n\u001b[1mDimensions must be equal, but are 32 and 64 for '{{node stgcn_2_1/graph_conv_layer_4_1/MatMul_1}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false, grad_x=false, grad_y=false](stgcn_2_1/graph_conv_layer_4_1/MatMul, stgcn_2_1/graph_conv_layer_4_1/MatMul_1/ReadVariableOp)' with input shapes: [1,19,32], [64,64].\u001b[0m\n\nArguments received by GraphConvLayer.call():\n  • inputs=tf.Tensor(shape=(1, 19, 32), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# ✅ 여러 개의 JSON 파일을 로드하고 모델 예측 수행\n",
    "def predict_multiple_json_skeleton(file_paths):\n",
    "    results = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # JSON 데이터 로드\n",
    "        X_data, _ = load_json_skeleton_view1(file_path)  # 기존의 JSON 로딩 함수 사용\n",
    "\n",
    "        # 모델 예측\n",
    "        prediction = stgcn_model.predict(X_data)\n",
    "        predicted_class = np.argmax(prediction, axis=1)[0]  # 0 = 올바른 자세, 1 = 잘못된 자세\n",
    "        confidence = prediction[0][predicted_class]  # 선택된 클래스의 확률 값\n",
    "\n",
    "        # ✅ 결과 저장\n",
    "        if predicted_class == 0:\n",
    "            results[file_path] = f\"✅ 올바른 자세 ({confidence * 100:.2f}% 확신)\"\n",
    "        else:\n",
    "            results[file_path] = f\"❌ 잘못된 자세 감지 ({confidence * 100:.2f}% 확신)\"\n",
    "\n",
    "        #     # ✅ 결과 저장\n",
    "        # if predicted_class == 0:\n",
    "        #     results[file_path] = f\"✅ 올바른 자세\"\n",
    "        # else:\n",
    "        #     results[file_path] = f\"❌ 잘못된 자세 감지\"\n",
    "\n",
    "    return results\n",
    "\n",
    "# ✅ 여러 개의 JSON 파일 리스트\n",
    "file_paths = [\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-562.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-563.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-564.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-565.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-566.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-567.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-568.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-569.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-570.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-571.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-572.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-573.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-574.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-575.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-576.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-577.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-578.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-579.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-580.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-581.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-582.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-583.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-584.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-585.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-586.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-587.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-588.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-589.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-590.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-591.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/grad/body_v-1-592.json\"\n",
    "]\n",
    "\n",
    "# ✅ 예측 결과 얻기\n",
    "prediction_results = predict_multiple_json_skeleton(file_paths)\n",
    "\n",
    "# ✅ 결과 출력\n",
    "for file, result in prediction_results.items():\n",
    "    print(f\"{file}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b0e9f-01d7-48ad-b83b-62231b8a7121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
