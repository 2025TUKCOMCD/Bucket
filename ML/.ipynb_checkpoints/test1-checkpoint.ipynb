{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "463ae2ff-2580-4da6-9a54-d3b05bb44c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 사용할 관절(Keypoints)\n",
    "# keypoints = [\n",
    "#     \"Nose\", \"Left Eye\", \"Right Eye\", \"Left Ear\", \"Right Ear\",\n",
    "#     \"Left Shoulder\", \"Right Shoulder\", \"Left Elbow\", \"Right Elbow\",\n",
    "#     \"Left Wrist\", \"Right Wrist\", \"Left Hip\", \"Right Hip\",\n",
    "#     \"Left Knee\", \"Right Knee\", \"Left Ankle\", \"Right Ankle\",\n",
    "#     \"Neck\", \"Left Palm\", \"Right Palm\", \"Back\", \"Waist\",\n",
    "#     \"Left Foot\", \"Right Foot\"\n",
    "# ]\n",
    "0, 2, 5, 7, 8, 11, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28\n",
    "keypoints = [\n",
    "    \"Point_0\", \"Point_2\", \"Point_5\", \"Point_7\", \"Point_8\", \"Point_11\", \n",
    "    \"Point_12\", \"Point_13\", \"Point_14\", \"Point_15\", \"Point_16\", \"Point_21\", \n",
    "    \"Point_22\", \"Point_23\", \"Point_24\", \"Point_25\", \"Point_26\", \"Point_27\", \"Point_28\"\n",
    "]\n",
    "\n",
    "\n",
    "# JSON 데이터 로드 함수\n",
    "def load_json_skeleton_view1(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    num_frames = len(data[\"frames\"])\n",
    "    num_joints = len(keypoints)\n",
    "    num_features = 2  # (x, y)\n",
    "    num_views = 1     \n",
    "\n",
    "    # ✅ (1, 프레임, 뷰, 관절, 좌표) 형태로 데이터 배열 생성\n",
    "    X_data = np.zeros((1, num_frames, num_views, num_joints, num_features), dtype=np.float32)\n",
    "\n",
    "    views = [\"view1\"]\n",
    "\n",
    "    # ✅ JSON 데이터 -> 배열 변환\n",
    "    for frame_idx, frame in enumerate(data[\"frames\"]):\n",
    "        for view_idx, view in enumerate(views):\n",
    "            pts = frame.get(view, {}).get(\"pts\", {})\n",
    "            for joint_idx, joint_name in enumerate(keypoints):\n",
    "                if joint_name in pts:\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 0] = pts[joint_name][\"x\"]\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 1] = pts[joint_name][\"y\"]\n",
    "\n",
    "    return X_data, data.get(\"type_info\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "b1c40f17-54a1-4684-a426-716833db587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 데이터 Shape: (80, 32, 5, 19, 2)\n"
     ]
    }
   ],
   "source": [
    "# ✅ JSON 데이터 로드 함수 (5개 각도 전처리)\n",
    "def load_json_skeleton(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    num_frames = len(data[\"frames\"])\n",
    "    num_joints = len(keypoints)\n",
    "    num_features = 2  # (x, y)\n",
    "    num_views = 5     # view1 ~ view5\n",
    "\n",
    "    # ✅ (1, 프레임, 뷰, 관절, 좌표) 형태로 데이터 배열 생성\n",
    "    X_data = np.zeros((1, num_frames, num_views, num_joints, num_features), dtype=np.float32)\n",
    "\n",
    "    views = [\"view1\", \"view2\", \"view3\", \"view4\", \"view5\"]\n",
    "\n",
    "    # ✅ JSON 데이터 -> 배열 변환\n",
    "    for frame_idx, frame in enumerate(data[\"frames\"]):\n",
    "        for view_idx, view in enumerate(views):\n",
    "            pts = frame.get(view, {}).get(\"pts\", {})\n",
    "            for joint_idx, joint_name in enumerate(keypoints):\n",
    "                if joint_name in pts:\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 0] = pts[joint_name][\"x\"]\n",
    "                    X_data[0, frame_idx, view_idx, joint_idx, 1] = pts[joint_name][\"y\"]\n",
    "\n",
    "    return X_data, data.get(\"type_info\", None)\n",
    "\n",
    "# ✅ 여러 개의 JSON 파일을 한 번에 로드하는 함수 (올바른/잘못된 데이터 포함)\n",
    "def load_labeled_json_skeleton(file_paths, labels):\n",
    "    X_data_list = []\n",
    "    y_data_list = []\n",
    "\n",
    "    for file_path, label in zip(file_paths, labels):\n",
    "        X, _ = load_json_skeleton(file_path)\n",
    "        X_data_list.append(X)\n",
    "        y_data_list.append(label)\n",
    "\n",
    "    # ✅ 여러 개의 파일을 하나의 NumPy 배열로 병합\n",
    "    X_train = np.concatenate(X_data_list, axis=0)  # (batch_size, frames, views, joints, features)\n",
    "    y_train = np.array(y_data_list)                # (batch_size, )\n",
    "\n",
    "    return X_train, y_train\n",
    "    \n",
    "# ✅ 올바른 자세와 잘못된 자세 데이터를 함께 로드\n",
    "file_paths = [\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body10-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body11-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body12-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body17-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body17-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body17-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body17-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body17-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-562.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-563.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-564.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-565.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-566.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-567.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-568.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-569.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-570.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-571.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-572.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-573.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-574.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-575.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-576.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-577.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-578.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-579.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-580.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-581.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-582.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-583.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-584.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-585.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-586.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-587.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-588.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-589.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-590.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-591.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body09-1-592.json\",    #31\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-562.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-563.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-564.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-565.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-566.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-567.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-568.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-569.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/1.Training/gradu/scaled/scaled_body08-1-570.json\"\n",
    "]\n",
    "\n",
    "labels = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1,]  # 0 = 올바른 자세, 1 = 잘못된 자세\n",
    "\n",
    "# ✅ 전처리 실행\n",
    "X_train, y_train = load_labeled_json_skeleton(file_paths, labels)\n",
    "# ✅ 전처리된 데이터 형태 확인\n",
    "print(\"전처리된 데이터 Shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "eb3eaa30-3fdb-4a25-ae7e-7692c4e52f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 생성된 adjacency_matrix 크기: (19, 19)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import mediapipe as mp\n",
    "\n",
    "# ✅ 그래프 컨볼루션 레이어 정의\n",
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(self, units, adjacency_matrix):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.adjacency_matrix = tf.Variable(adjacency_matrix, dtype=tf.float32, trainable=False)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.linalg.matmul(self.adjacency_matrix, inputs)  # (batch, joints, features)\n",
    "        x = tf.linalg.matmul(x, self.kernel)  # 가중치 적용\n",
    "        return tf.nn.leaky_relu(x)  # 활성화 함수 적용\n",
    "\n",
    "# ✅ ST-GCN 모델 정의\n",
    "class STGCN(tf.keras.Model):\n",
    "    def __init__(self, num_joints, num_features, adjacency_matrix, num_classes):\n",
    "        super(STGCN, self).__init__()\n",
    "        self.graph_conv1 = GraphConvLayer(64, adjacency_matrix)\n",
    "        self.graph_conv2 = GraphConvLayer(128, adjacency_matrix)\n",
    "        self.graph_conv3 = GraphConvLayer(256, adjacency_matrix)  # 추가된 Graph Conv\n",
    "        self.graph_conv4 = GraphConvLayer(512, adjacency_matrix)  # 추가된 Graph Conv\n",
    "        self.temporal_conv1 = layers.Conv1D(512, kernel_size=5, padding=\"same\")\n",
    "        self.temporal_conv2 = layers.Conv1D(256, kernel_size=5, padding=\"same\")\n",
    "        self.temporal_conv3 = layers.Conv1D(128, kernel_size=5, padding=\"same\")\n",
    "        self.temporal_conv4 = layers.Conv1D(64, kernel_size=5, padding=\"same\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "        self.batch_norm3 = layers.BatchNormalization()\n",
    "        self.batch_norm4 = layers.BatchNormalization()\n",
    "        self.activation = layers.Activation(\"relu\")\n",
    "        self.fc = layers.Dense(num_classes, activation=\"softmax\")\n",
    "        self.dropout = layers.Dropout(0.5) \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # ✅ 입력 처리: (batch, frames, views, joints, features)\n",
    "        if len(inputs.shape) == 5:\n",
    "            # 여러 각도(View) 데이터가 있는 경우 평균 내기\n",
    "            inputs = tf.reduce_mean(inputs, axis=2)  # (batch, frames, joints, features)\n",
    "        \n",
    "        batch_size, frames, joints, features = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n",
    "       \n",
    "        # inputs = tf.reshape(inputs, (batch_size, frames, joints * features))\n",
    "        \n",
    "        # # ✅ 차원 재정렬: (batch, joints, frames, features)\n",
    "        # inputs = tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "        # inputs = tf.reshape(inputs, (batch_size, joints, frames * features))\n",
    "\n",
    "        inputs = tf.reshape(inputs, (batch_size, frames, joints, features))  # (batch, frames, joints, features)\n",
    "        inputs = tf.transpose(inputs, perm=[0, 2, 1, 3])  # (batch, joints, frames, features)\n",
    "        inputs = tf.reshape(inputs, (batch_size * frames, joints, features))  # (batch * frames, joints, features)\n",
    "\n",
    "\n",
    "        # print(X_train.shape)\n",
    "\n",
    "        # ✅ 모델 처리\n",
    "        x = self.graph_conv1(inputs)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.graph_conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.graph_conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.graph_conv4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.temporal_conv1(x)\n",
    "        x = self.temporal_conv2(x)\n",
    "        x = self.temporal_conv3(x)\n",
    "        x = self.temporal_conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.fc(x)\n",
    "\n",
    "# ✅ 그래프 인접 행렬 (단순 단위 행렬)\n",
    "num_joints = len(keypoints)\n",
    "num_features = 2\n",
    "num_classes = 2  # (올바른 자세 / 잘못된 자세)\n",
    "# adjacency_matrix = np.identity(num_joints)\n",
    "\n",
    "# Mediapipe에서 제공하는 POSE_CONNECTIONS을 활용\n",
    "mp_pose = mp.solutions.pose\n",
    "connections = list(mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "# ✅ 현재 선택된 19개 관절 인덱스 (사용자가 선택한 관절 리스트)\n",
    "selected_joints = [0, 2, 5, 7, 8, 11, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "# ✅ 원본 33개 관절을 기반으로 한 인접 행렬 생성\n",
    "full_adjacency_matrix = np.zeros((33, 33))  # 전체 33개 관절을 사용한 경우\n",
    "for joint1, joint2 in connections:\n",
    "    full_adjacency_matrix[joint1, joint2] = 1\n",
    "    full_adjacency_matrix[joint2, joint1] = 1  # 대칭 관계\n",
    "\n",
    "# ✅ 선택된 19개 관절만을 포함하는 인접 행렬 생성\n",
    "num_joints = len(selected_joints)\n",
    "adjacency_matrix = np.zeros((num_joints, num_joints))\n",
    "\n",
    "for i, joint1 in enumerate(selected_joints):\n",
    "    for j, joint2 in enumerate(selected_joints):\n",
    "        adjacency_matrix[i, j] = full_adjacency_matrix[joint1, joint2]  # 기존 인접 행렬에서 추출\n",
    "\n",
    "print(f\"✅ 생성된 adjacency_matrix 크기: {adjacency_matrix.shape}\")  # (19, 19) 확인\n",
    "\n",
    "# ✅ ST-GCN 모델 생성 및 컴파일\n",
    "del stgcn_model\n",
    "stgcn_model = STGCN(num_joints, num_features, adjacency_matrix, num_classes)\n",
    "stgcn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "5725513f-47a7-48ec-bfb8-265738ac83ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5375 - loss: 1945.3699\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5000 - loss: 3262.9575\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5000 - loss: 4583.1782\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5000 - loss: 4848.6797\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5000 - loss: 4516.6323\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.5000 - loss: 3832.8354\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5000 - loss: 2942.3691\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5000 - loss: 1931.9000\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.5000 - loss: 849.9352\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5625 - loss: 329.5685\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 1.1682e-06\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 1.1667e-06\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 1.1652e-06\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 1.1652e-06\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 1.1623e-06\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 1.1623e-06\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 1.1593e-06\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 1.1593e-06\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 1.1578e-06\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 1.1563e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x199dc163ef0>"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "stgcn_model.fit(X_train, y_train, epochs=10, batch_size=80, verbose=1)\n",
    "stgcn_model.fit(X_train, y_train, epochs=1000, batch_size=80, verbose=0)\n",
    "stgcn_model.fit(X_train, y_train, epochs=10, batch_size=80, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "cb185c6a-391e-489c-aa8c-94ef81e2b9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-561.json: ✅ 올바른 자세 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-2-561.json: ✅ 올바른 자세 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-3-561.json: ✅ 올바른 자세 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-4-561.json: ✅ 올바른 자세 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-5-561.json: ✅ 올바른 자세 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-6-561.json: ✅ 올바른 자세 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-7-561.json: ✅ 올바른 자세 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-562.json: ✅ 올바른 자세 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-563.json: ❌ 잘못된 자세 감지 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-564.json: ❌ 잘못된 자세 감지 (99.97% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-565.json: ✅ 올바른 자세 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-566.json: ❌ 잘못된 자세 감지 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-567.json: ✅ 올바른 자세 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-568.json: ❌ 잘못된 자세 감지 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-569.json: ❌ 잘못된 자세 감지 (100.00% 확신)\n",
      "D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-570.json: ✅ 올바른 자세 (100.00% 확신)\n"
     ]
    }
   ],
   "source": [
    "# ✅ 여러 개의 JSON 파일을 로드하고 모델 예측 수행\n",
    "def predict_multiple_json_skeleton(file_paths):\n",
    "    results = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # JSON 데이터 로드\n",
    "        X_data, _ = load_json_skeleton(file_path)  # 기존의 JSON 로딩 함수 사용\n",
    "\n",
    "        \n",
    "        # # ✅ View 차원 평균 제거 및 변환을 `fit()` 실행 전에 처리\n",
    "        # X_data = np.mean(X_data, axis=2)  # (80, 32, 19, 2)\n",
    "        # X_data = np.transpose(X_data, (0, 2, 1, 3))  # (80, 19, 32, 2)\n",
    "        # batch_size, joints, frames, features = X_data.shape\n",
    "        # X_data = np.reshape(X_data, (batch_size, joints, frames * features))  # (80, 19, 64)\n",
    "        \n",
    "        # # ✅ Tensor 변환\n",
    "        # X_data = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "\n",
    "        # 모델 예측\n",
    "        prediction = stgcn_model.predict(X_data)\n",
    "        predicted_class = np.argmax(prediction, axis=1)[0]  # 0 = 올바른 자세, 1 = 잘못된 자세\n",
    "        confidence = prediction[0][predicted_class]  # 선택된 클래스의 확률 값\n",
    "\n",
    "        # ✅ 결과 저장\n",
    "        if predicted_class == 0:\n",
    "            results[file_path] = f\"✅ 올바른 자세 ({confidence * 100:.2f}% 확신)\"\n",
    "        else:\n",
    "            results[file_path] = f\"❌ 잘못된 자세 감지 ({confidence * 100:.2f}% 확신)\"\n",
    "\n",
    "        #     # ✅ 결과 저장\n",
    "        # if predicted_class == 0:\n",
    "        #     results[file_path] = f\"✅ 올바른 자세\"\n",
    "        # else:\n",
    "        #     results[file_path] = f\"❌ 잘못된 자세 감지\"\n",
    "\n",
    "    return results\n",
    "\n",
    "# ✅ 여러 개의 JSON 파일 리스트\n",
    "file_paths = [\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-2-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-3-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-4-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-5-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-6-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-7-561.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-562.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-563.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-564.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-565.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-566.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-567.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-568.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-569.json\",\n",
    "    \"D:/Studying/gradu/013.피트니스자세/2.Validation/검증데이터/scaled/scaled_body_v-1-570.json\"\n",
    "]\n",
    "\n",
    "# ✅ 예측 결과 얻기\n",
    "prediction_results = predict_multiple_json_skeleton(file_paths)\n",
    "\n",
    "# ✅ 결과 출력\n",
    "for file, result in prediction_results.items():\n",
    "    print(f\"{file}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
