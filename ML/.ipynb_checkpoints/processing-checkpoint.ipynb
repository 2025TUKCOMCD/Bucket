{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abf520d6-147d-4913-8a3a-6e6a3b642bdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] ì§€ì •ëœ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: 'D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_10/gradu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# ì„œë¸Œ í´ë” ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 5ê°œ ì„ íƒ)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m json_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m593\u001b[39m:\n\u001b[1;32m---> 28\u001b[0m     subfolders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(parent_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mis_dir()])[s_idx:e_idx]\n\u001b[0;32m     29\u001b[0m     s_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     30\u001b[0m     e_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] ì§€ì •ëœ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: 'D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_10/gradu'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Mediapipe Pose ëª¨ë¸ ë¡œë“œ\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "selected_landmarks = [0, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "\n",
    "data = {\n",
    "    \"frames\": []\n",
    "}\n",
    "\n",
    "s_idx, e_idx = 0, 5\n",
    "json_count = 562\n",
    "\n",
    "# ë¶€ëª¨ í´ë” (ì„œë¸Œ í´ë”ê°€ ì¡´ì¬í•˜ëŠ” í´ë”)\n",
    "parent_folder = \"D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_09/gradu\"\n",
    "json_output_path = \"D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/gradu/body09-1-562.json\"\n",
    "\n",
    "# ì„œë¸Œ í´ë” ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 5ê°œ ì„ íƒ)\n",
    "while json_count < 593:\n",
    "    subfolders = sorted([f.path for f in os.scandir(parent_folder) if f.is_dir()])[s_idx:e_idx]\n",
    "    s_idx += 5\n",
    "    e_idx += 5\n",
    "    \n",
    "    # Mediapipe Pose ëª¨ë¸ ì‹¤í–‰\n",
    "    with mp_pose.Pose(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5, enable_segmentation=False) as pose:\n",
    "        for folder_idx, subfolder in enumerate(subfolders):\n",
    "            view_name = f\"view{folder_idx+1}\"  # view1, view2, ..., view5\n",
    "    \n",
    "            # ì„œë¸Œ í´ë” ë‚´ ì´ë¯¸ì§€ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°\n",
    "            image_files = sorted(glob(os.path.join(subfolder, \"*.jpg\")))\n",
    "    \n",
    "            if not image_files:\n",
    "                print(f\"âš ï¸ {subfolder} ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                continue\n",
    "    \n",
    "            print(f\"ğŸ“‚ Processing folder for {view_name}: {subfolder}\")\n",
    "    \n",
    "            # í•œ ì„œë¸Œí´ë” ë‚´ 32ê°œ ì´ë¯¸ì§€ ì €ì¥\n",
    "            for img_idx, img_file in enumerate(image_files[:32]):  # 32ê°œ ì´ë¯¸ì§€ ì‚¬ìš©\n",
    "                try:\n",
    "                    # PILì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¡œë“œ\n",
    "                    image = Image.open(img_file).convert(\"RGB\")\n",
    "                    \n",
    "                    # print(f\"âœ… ì •ìƒ ë¡œë“œ: {img_file}\")\n",
    "                    image_np = np.array(image)\n",
    "    \n",
    "                    # Mediapipeì— ì…ë ¥í•  ìˆ˜ ìˆë„ë¡ ë°°ì—´ ë³€í™˜ (H, W, C ìˆœì„œ ìœ ì§€)\n",
    "                    results = pose.process(image_np)\n",
    "    \n",
    "                    # í‚¤í¬ì¸íŠ¸ ì¢Œí‘œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ (ì •ê·œí™”ëœ ê°’ ìœ ì§€)\n",
    "                    keypoints = {}\n",
    "    \n",
    "                    if results.pose_landmarks:\n",
    "                        for idx in selected_landmarks: \n",
    "                            landmark = results.pose_landmarks.landmark[idx]\n",
    "                            keypoints[f\"Point_{idx}\"] = {\n",
    "                                \"x\": landmark.x,  # ì •ê·œí™”ëœ ê°’ (0~1)\n",
    "                                \"y\": landmark.y   # ì •ê·œí™”ëœ ê°’ (0~1)\n",
    "                            }\n",
    "    \n",
    "                    # JSON êµ¬ì¡°ì— view ì¶”ê°€\n",
    "                    frame_data = {\n",
    "                        \"pts\": keypoints,\n",
    "                        \"active\": \"Yes\" if results.pose_landmarks else \"No\",\n",
    "                        \"img_key\": img_file\n",
    "                    }\n",
    "    \n",
    "                    # í•œ í”„ë ˆì„ ë‚´ì— 5ê°œì˜ view í¬í•¨\n",
    "                    if len(data[\"frames\"]) <= img_idx:\n",
    "                        data[\"frames\"].append({})  # ì´ë¯¸ì§€ ê°œìˆ˜ë§Œí¼ í”„ë ˆì„ ìƒì„±\n",
    "    \n",
    "                    data[\"frames\"][img_idx][view_name] = frame_data\n",
    "    \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜ ({img_file}): {e}\")\n",
    "                    continue\n",
    "    \n",
    "    with open(json_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… 5ê°œ View JSON íŒŒì¼ì´ {json_output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    json_count += 1\n",
    "    json_output_path = json_output_path.replace(str(json_count - 1), str(json_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9903e0c-2348-4306-964f-0f305379552b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Processing folder for view1: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/1\\561-1-3-27-Z37_A\n",
      "ğŸ“‚ Processing folder for view2: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/1\\561-1-3-27-Z37_B\n",
      "ğŸ“‚ Processing folder for view3: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/1\\561-1-3-27-Z37_C\n",
      "ğŸ“‚ Processing folder for view4: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/1\\561-1-3-27-Z37_D\n",
      "ğŸ“‚ Processing folder for view5: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/1\\561-1-3-27-Z37_E\n",
      "âœ… 5ê°œ View JSON íŒŒì¼ì´ D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/gradu/body17-1-561.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“‚ Processing folder for view1: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/2\\561-1-3-27-Z54_A\n",
      "ğŸ“‚ Processing folder for view2: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/2\\561-1-3-27-Z54_B\n",
      "ğŸ“‚ Processing folder for view3: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/2\\561-1-3-27-Z54_C\n",
      "ğŸ“‚ Processing folder for view4: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/2\\561-1-3-27-Z54_D\n",
      "ğŸ“‚ Processing folder for view5: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/2\\561-1-3-27-Z54_E\n",
      "âœ… 5ê°œ View JSON íŒŒì¼ì´ D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/gradu/body17-2-561.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“‚ Processing folder for view1: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/3\\561-1-3-27-Z98_A\n",
      "ğŸ“‚ Processing folder for view2: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/3\\561-1-3-27-Z98_B\n",
      "ğŸ“‚ Processing folder for view3: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/3\\561-1-3-27-Z98_C\n",
      "ğŸ“‚ Processing folder for view4: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/3\\561-1-3-27-Z98_D\n",
      "ğŸ“‚ Processing folder for view5: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/3\\561-1-3-27-Z98_E\n",
      "âœ… 5ê°œ View JSON íŒŒì¼ì´ D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/gradu/body17-3-561.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“‚ Processing folder for view1: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/4\\561-1-3-27-Z131_A\n",
      "ğŸ“‚ Processing folder for view2: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/4\\561-1-3-27-Z131_B\n",
      "ğŸ“‚ Processing folder for view3: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/4\\561-1-3-27-Z131_C\n",
      "ğŸ“‚ Processing folder for view4: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/4\\561-1-3-27-Z131_D\n",
      "ğŸ“‚ Processing folder for view5: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/4\\561-1-3-27-Z131_E\n",
      "âœ… 5ê°œ View JSON íŒŒì¼ì´ D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/gradu/body17-4-561.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“‚ Processing folder for view1: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/5\\561-1-3-27-Z57_A\n",
      "ğŸ“‚ Processing folder for view2: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/5\\561-1-3-27-Z57_B\n",
      "ğŸ“‚ Processing folder for view3: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/5\\561-1-3-27-Z57_C\n",
      "ğŸ“‚ Processing folder for view4: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/5\\561-1-3-27-Z57_D\n",
      "ğŸ“‚ Processing folder for view5: D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/5\\561-1-3-27-Z57_E\n",
      "âœ… 5ê°œ View JSON íŒŒì¼ì´ D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/gradu/body17-5-561.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "s_idx, e_idx = 0, 5\n",
    "parent_folder = \"D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/ì›ì‹œë°ì´í„°/body_17/1\"\n",
    "json_output_path = \"D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/1.Training/gradu/body17-1-561.json\"\n",
    "\n",
    "for i in range(5):\n",
    "    subfolders = sorted([f.path for f in os.scandir(parent_folder) if f.is_dir()])[s_idx:e_idx]\n",
    "    \n",
    "    # Mediapipe Pose ëª¨ë¸ ì‹¤í–‰\n",
    "    with mp_pose.Pose(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5, enable_segmentation=False) as pose:\n",
    "        for folder_idx, subfolder in enumerate(subfolders):\n",
    "            view_name = f\"view{folder_idx+1}\"  # view1, view2, ..., view5\n",
    "    \n",
    "            # ì„œë¸Œ í´ë” ë‚´ ì´ë¯¸ì§€ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°\n",
    "            image_files = sorted(glob(os.path.join(subfolder, \"*.jpg\")))\n",
    "    \n",
    "            if not image_files:\n",
    "                print(f\"âš ï¸ {subfolder} ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                continue\n",
    "    \n",
    "            print(f\"ğŸ“‚ Processing folder for {view_name}: {subfolder}\")\n",
    "    \n",
    "            # í•œ ì„œë¸Œí´ë” ë‚´ 32ê°œ ì´ë¯¸ì§€ ì €ì¥\n",
    "            for img_idx, img_file in enumerate(image_files[:32]):  # 32ê°œ ì´ë¯¸ì§€ ì‚¬ìš©\n",
    "                try:\n",
    "                    # PILì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¡œë“œ\n",
    "                    image = Image.open(img_file).convert(\"RGB\")\n",
    "                    \n",
    "                    # print(f\"âœ… ì •ìƒ ë¡œë“œ: {img_file}\")\n",
    "                    image_np = np.array(image)\n",
    "    \n",
    "                    # Mediapipeì— ì…ë ¥í•  ìˆ˜ ìˆë„ë¡ ë°°ì—´ ë³€í™˜ (H, W, C ìˆœì„œ ìœ ì§€)\n",
    "                    results = pose.process(image_np)\n",
    "    \n",
    "                    # í‚¤í¬ì¸íŠ¸ ì¢Œí‘œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ (ì •ê·œí™”ëœ ê°’ ìœ ì§€)\n",
    "                    keypoints = {}\n",
    "    \n",
    "                    if results.pose_landmarks:\n",
    "                        for idx in selected_landmarks:  # 19ê°œ ê´€ì ˆë§Œ ì„ íƒ\n",
    "                            landmark = results.pose_landmarks.landmark[idx]\n",
    "                            keypoints[f\"Point_{idx}\"] = {\n",
    "                                \"x\": landmark.x,  # ì •ê·œí™”ëœ ê°’ (0~1)\n",
    "                                \"y\": landmark.y   # ì •ê·œí™”ëœ ê°’ (0~1)\n",
    "                            }\n",
    "    \n",
    "                    # JSON êµ¬ì¡°ì— view ì¶”ê°€\n",
    "                    frame_data = {\n",
    "                        \"pts\": keypoints,\n",
    "                        \"active\": \"Yes\" if results.pose_landmarks else \"No\",\n",
    "                        \"img_key\": img_file\n",
    "                    }\n",
    "    \n",
    "                    # í•œ í”„ë ˆì„ ë‚´ì— 5ê°œì˜ view í¬í•¨\n",
    "                    if len(data[\"frames\"]) <= img_idx:\n",
    "                        data[\"frames\"].append({})  # ì´ë¯¸ì§€ ê°œìˆ˜ë§Œí¼ í”„ë ˆì„ ìƒì„±\n",
    "    \n",
    "                    data[\"frames\"][img_idx][view_name] = frame_data\n",
    "    \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜ ({img_file}): {e}\")\n",
    "                    continue\n",
    "    \n",
    "    with open(json_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… 5ê°œ View JSON íŒŒì¼ì´ {json_output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    parent_folder = parent_folder.replace(f\"body_17/{str(i + 1)}\", f\"body_17/{str(i + 2)}\")\n",
    "    json_output_path = json_output_path.replace(f\"body17-{str(i + 1)}\", f\"body17-{str(i + 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a418db5-f389-41db-811b-ff16558d48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import glob\n",
    "# import math\n",
    "# import numpy as np\n",
    "\n",
    "# DEFAULT_IMAGE_WIDTH = 1280\n",
    "# DEFAULT_IMAGE_HEIGHT = 960\n",
    "# MIN_SHOULDER_WIDTH = 20  # ìµœì†Œ ì–´ê¹¨ ë„ˆë¹„ ì„¤ì •\n",
    "\n",
    "# def compute_avg_shoulder_width(json_files):\n",
    "#     widths = []\n",
    "#     for json_file in json_files:\n",
    "#         with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#             data = json.load(f)\n",
    "#             for frame in data[\"frames\"]:\n",
    "#                 for view in frame:\n",
    "#                     if \"pts\" in frame[view]:\n",
    "#                         keypoints = frame[view][\"pts\"]\n",
    "#                         left_shoulder = keypoints.get(\"Point_11\", {\"x\": None})\n",
    "#                         right_shoulder = keypoints.get(\"Point_12\", {\"x\": None})\n",
    "#                         if left_shoulder[\"x\"] and right_shoulder[\"x\"]:\n",
    "#                             widths.append(abs(left_shoulder[\"x\"] - right_shoulder[\"x\"]))\n",
    "#     return np.mean(widths) if widths else 1.0\n",
    "\n",
    "# def scale_keypoints_using_shoulder(data, avg_shoulder_width, image_width=DEFAULT_IMAGE_WIDTH, image_height=DEFAULT_IMAGE_HEIGHT):\n",
    "#     prev_x, prev_y = 0, 0 \n",
    "\n",
    "#     for frame in data[\"frames\"]:\n",
    "#         for view in frame:\n",
    "#             if \"pts\" in frame[view]:  \n",
    "#                 keypoints = frame[view][\"pts\"]\n",
    "\n",
    "#                 left_shoulder = keypoints.get(\"Point_11\", {\"x\": None, \"y\": None})\n",
    "#                 right_shoulder = keypoints.get(\"Point_12\", {\"x\": None, \"y\": None})\n",
    "\n",
    "#                 if left_shoulder[\"x\"] is None or right_shoulder[\"x\"] is None:\n",
    "#                     continue  \n",
    "\n",
    "#                 left_shoulder_x = left_shoulder[\"x\"] * image_width\n",
    "#                 right_shoulder_x = right_shoulder[\"x\"] * image_width\n",
    "\n",
    "#                 shoulder_width = abs(left_shoulder_x - right_shoulder_x) or avg_shoulder_width\n",
    "#                 if shoulder_width < MIN_SHOULDER_WIDTH:\n",
    "#                     shoulder_width = MIN_SHOULDER_WIDTH\n",
    "\n",
    "#                 for point in keypoints:\n",
    "#                     if keypoints[point][\"x\"] is None or keypoints[point][\"y\"] is None:\n",
    "#                         continue  \n",
    "\n",
    "#                     if math.isnan(keypoints[point][\"x\"]) or math.isnan(keypoints[point][\"y\"]):\n",
    "#                         keypoints[point][\"x\"], keypoints[point][\"y\"] = prev_x, prev_y  \n",
    "\n",
    "#                     prev_x, prev_y = keypoints[point][\"x\"], keypoints[point][\"y\"]\n",
    "\n",
    "#                 for point in keypoints:\n",
    "#                     keypoints[point][\"x\"] = (keypoints[point][\"x\"] * image_width) / shoulder_width\n",
    "#                     keypoints[point][\"y\"] = (keypoints[point][\"y\"] * image_height) / shoulder_width\n",
    "                    \n",
    "#     return data\n",
    "\n",
    "# input_folder = \"D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/2.Validation/ê²€ì¦ë°ì´í„°\"\n",
    "# output_folder = \"D:/Studying/gradu/013.í”¼íŠ¸ë‹ˆìŠ¤ìì„¸/2.Validation/ê²€ì¦ë°ì´í„°/scaled/\"\n",
    "\n",
    "# if not os.path.exists(output_folder):\n",
    "#     os.makedirs(output_folder)\n",
    "\n",
    "# json_files = glob.glob(os.path.join(input_folder, \"*.json\"))\n",
    "# avg_shoulder_width = compute_avg_shoulder_width(json_files)\n",
    "\n",
    "# for json_file in json_files:\n",
    "#     try:\n",
    "#         with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#             json_data = json.load(f)\n",
    "\n",
    "#         scaled_data = scale_keypoints_using_shoulder(json_data, avg_shoulder_width)\n",
    "\n",
    "#         file_name = os.path.basename(json_file)\n",
    "#         output_json_path = os.path.join(output_folder, f\"scaled_{file_name}\")\n",
    "\n",
    "#         with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#             json.dump(scaled_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "#         print(f\"âœ… ë³€í™˜ ì™„ë£Œ: {output_json_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ íŒŒì¼ ë³€í™˜ ì˜¤ë¥˜: {json_file}, ì˜¤ë¥˜ ë‚´ìš©: {e}\")\n",
    "\n",
    "# print(\"ğŸ¯ ëª¨ë“  JSON íŒŒì¼ì˜ ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
